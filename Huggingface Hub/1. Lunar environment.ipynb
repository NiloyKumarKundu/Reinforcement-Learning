{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3284bfb0-9ca0-41bf-a692-e5f3b6210e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "from huggingface_sb3 import load_from_hub, package_to_hub\n",
    "from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01337f45-cfa9-4916-a4d6-59f029f862ff",
   "metadata": {},
   "source": [
    "## Understand Gymnasium and how it works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938d4a53-0d30-4fea-96c8-a5f1edd60f30",
   "metadata": {},
   "source": [
    "At each step:\n",
    "- Our Agent receives¬†a **state (S0)**¬†from the¬†**Environment**¬†‚Äî we receive the first frame of our game (Environment).\n",
    "- Based on that¬†**state (S0),**¬†the Agent takes an¬†**action (A0)**¬†‚Äî our Agent will move to the right.\n",
    "- The environment transitions to a¬†**new**¬†**state (S1)**¬†‚Äî new frame.\n",
    "- The environment gives some¬†**reward (R1)**¬†to the Agent ‚Äî we‚Äôre not dead¬†*(Positive Reward +1)*.\n",
    "\n",
    "\n",
    "With Gymnasium:\n",
    "\n",
    "1Ô∏è‚É£ We create our environment using `gymnasium.make()`\n",
    "\n",
    "2Ô∏è‚É£ We reset the environment to its initial state with `observation = env.reset()`\n",
    "\n",
    "At each step:\n",
    "\n",
    "3Ô∏è‚É£ Get an action using our model (in our example we take a random action)\n",
    "\n",
    "4Ô∏è‚É£ Using `env.step(action)`, we perform this action in the environment and get\n",
    "- `observation`: The new state (st+1)\n",
    "- `reward`: The reward we get after executing the action\n",
    "- `terminated`: Indicates if the episode terminated (agent reach the terminal state)\n",
    "- `truncated`: Introduced with this new version, it indicates a timelimit or if an agent go out of bounds of the environment for instance.\n",
    "- `info`: A dictionary that provides additional information (depends on the environment).\n",
    "\n",
    "For more explanations check this üëâ https://gymnasium.farama.org/api/env/#gymnasium.Env.step\n",
    "\n",
    "If the episode is terminated:\n",
    "- We reset the environment to its initial state with `observation = env.reset()`\n",
    "\n",
    "**Let's look at an example!** Make sure to read the code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ba150d5-15fb-4596-8e33-72d749bf87a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action taken: 2\n",
      "Action taken: 2\n",
      "Action taken: 2\n",
      "Action taken: 1\n",
      "Action taken: 3\n",
      "Action taken: 3\n",
      "Action taken: 1\n",
      "Action taken: 3\n",
      "Action taken: 1\n",
      "Action taken: 0\n",
      "Action taken: 3\n",
      "Action taken: 0\n",
      "Action taken: 0\n",
      "Action taken: 3\n",
      "Action taken: 2\n",
      "Action taken: 2\n",
      "Action taken: 1\n",
      "Action taken: 1\n",
      "Action taken: 2\n",
      "Action taken: 0\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# First, we create our environment called LunarLander-v2\n",
    "env = gym.make(\"LunarLander-v2\", render_mode='human')\n",
    "\n",
    "# Then we reset this environment\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(20):\n",
    "  # Take a random action\n",
    "  action = env.action_space.sample()\n",
    "  print(\"Action taken:\", action)\n",
    "\n",
    "  # Do this action in the environment and get\n",
    "  # next_state, reward, terminated, truncated and info\n",
    "  observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "  # If the game is terminated (in our case we land, crashed) or truncated (timeout)\n",
    "  if terminated or truncated:\n",
    "      # Reset the environment\n",
    "      print(\"Environment is reset\")\n",
    "      observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f20be72-1a5c-47c1-bf66-17a820ecc419",
   "metadata": {},
   "source": [
    "## Create the LunarLander environment üåõ and understand how it works\n",
    "\n",
    "### [The environment üéÆ](https://gymnasium.farama.org/environments/box2d/lunar_lander/)\n",
    "\n",
    "In this first tutorial, we‚Äôre going to train our agent, a [Lunar Lander](https://gymnasium.farama.org/environments/box2d/lunar_lander/), **to land correctly on the moon**. To do that, the agent needs to learn **to adapt its speed and position (horizontal, vertical, and angular) to land correctly.**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "üí° A good habit when you start to use an environment is to check its documentation\n",
    "\n",
    "üëâ https://gymnasium.farama.org/environments/box2d/lunar_lander/\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ae113cf-8e7e-444e-b9ba-c0985e8a0337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____OBSERVATION SPACE_____ \n",
      "\n",
      "Observation Space Shape (8,)\n",
      "Sample observation [-0.37528288  0.10197852  0.05969819  1.1791997   2.9339576   4.036679\n",
      "  0.9019675   0.43382293]\n"
     ]
    }
   ],
   "source": [
    "# We create our environment with gym.make(\"<name_of_the_environment>\")\n",
    "env = gym.make(\"LunarLander-v2\", render_mode='rgb_array')\n",
    "env.reset()\n",
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"Observation Space Shape\", env.observation_space.shape)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffabf889-c1cd-4956-bffa-be3c255812f4",
   "metadata": {},
   "source": [
    "We see with `Observation Space Shape (8,)` that the observation is a vector of size 8, where each value contains different information about the lander:\n",
    "- Horizontal pad coordinate (x)\n",
    "- Vertical pad coordinate (y)\n",
    "- Horizontal speed (x)\n",
    "- Vertical speed (y)\n",
    "- Angle\n",
    "- Angular speed\n",
    "- If the left leg contact point has touched the land (boolean)\n",
    "- If the right leg contact point has touched the land (boolean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3aa37ab4-dc04-4e94-a30a-5a59d8a7149c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " _____ACTION SPACE_____ \n",
      "\n",
      "Action Space Shape 4\n",
      "Action Space Sample 0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"Action Space Shape\", env.action_space.n)\n",
    "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3576da52-826b-48f0-b514-7f6ed8b61c3c",
   "metadata": {},
   "source": [
    "The action space (the set of possible actions the agent can take) is discrete with 4 actions available üéÆ:\n",
    "\n",
    "- Action 0: Do nothing,\n",
    "- Action 1: Fire left orientation engine,\n",
    "- Action 2: Fire the main engine,\n",
    "- Action 3: Fire right orientation engine.\n",
    "\n",
    "Reward function (the function that will give a reward at each timestep) üí∞:\n",
    "\n",
    "After every step a reward is granted. The total reward of an episode is the **sum of the rewards for all the steps within that episode**.\n",
    "\n",
    "For each step, the reward:\n",
    "\n",
    "- Is increased/decreased the closer/further the lander is to the landing pad.\n",
    "-  Is increased/decreased the slower/faster the lander is moving.\n",
    "- Is decreased the more the lander is tilted (angle not horizontal).\n",
    "- Is increased by 10 points for each leg that is in contact with the ground.\n",
    "- Is decreased by 0.03 points each frame a side engine is firing.\n",
    "- Is decreased by 0.3 points each frame the main engine is firing.\n",
    "\n",
    "The episode receive an **additional reward of -100 or +100 points for crashing or landing safely respectively.**\n",
    "\n",
    "An episode is **considered a solution if it scores at least 200 points.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff6e834-495d-400c-9d90-6e4de4a31e40",
   "metadata": {},
   "source": [
    "#### Vectorized Environment\n",
    "\n",
    "- We create a vectorized environment (a method for stacking multiple independent environments into a single environment) of 16 environments, this way, **we'll have more diverse experiences during the training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75b1396d-68fa-4cfd-8d70-7de775484933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = make_vec_env('LunarLander-v2', n_envs=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3cc455-7ffa-4758-8329-c65dd9088a2f",
   "metadata": {},
   "source": [
    "## Create the Model ü§ñ\n",
    "- We have studied our environment and we understood the problem: **being able to land the Lunar Lander to the Landing Pad correctly by controlling left, right and main orientation engine**. Now let's build the algorithm we're going to use to solve this Problem üöÄ.\n",
    "\n",
    "- To do so, we're going to use our first Deep RL library, [Stable Baselines3 (SB3)](https://stable-baselines3.readthedocs.io/en/master/).\n",
    "\n",
    "- SB3 is a set of **reliable implementations of reinforcement learning algorithms in PyTorch**.\n",
    "\n",
    "---\n",
    "\n",
    "üí° A good habit when using a new library is to dive first on the documentation: https://stable-baselines3.readthedocs.io/en/master/ and then try some tutorials.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4513d7-6ddd-405b-ad37-751f8951059b",
   "metadata": {},
   "source": [
    "To solve this problem, we're going to use SB3 **PPO**. [PPO (aka Proximal Policy Optimization) is one of the SOTA (state of the art) Deep Reinforcement Learning algorithms that you'll study during this course](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#example%5D).\n",
    "\n",
    "PPO is a combination of:\n",
    "- *Value-based reinforcement learning method*: learning an action-value function that will tell us the **most valuable action to take given a state and action**.\n",
    "- *Policy-based reinforcement learning method*: learning a policy that will **give us a probability distribution over actions**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e63310-90ee-4a38-98ca-3400c46787aa",
   "metadata": {},
   "source": [
    "Stable-Baselines3 is easy to set up:\n",
    "\n",
    "1Ô∏è‚É£ You **create your environment** (in our case it was done above)\n",
    "\n",
    "2Ô∏è‚É£ You define the **model you want to use and instantiate this model** `model = PPO(\"MlpPolicy\")`\n",
    "\n",
    "3Ô∏è‚É£ You **train the agent** with `model.learn` and define the number of training timesteps\n",
    "\n",
    "```\n",
    "# Create environment\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "# Instantiate the agent\n",
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=int(2e5))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b72c52ad-5f4e-4dfd-b2e7-fa502ac67d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# TODO: Define a PPO MlpPolicy architecture\n",
    "# We use MultiLayerPerceptron (MLPPolicy) because the input is a vector,\n",
    "# if we had frames as input we would use CnnPolicy\n",
    "\n",
    "model = PPO(\n",
    "    policy = 'MlpPolicy',\n",
    "    env = env,\n",
    "    n_steps = 1024,\n",
    "    batch_size = 64,\n",
    "    n_epochs = 4,\n",
    "    gamma = 0.999,\n",
    "    gae_lambda = 0.98,\n",
    "    ent_coef = 0.01,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f6e4b3-7965-4218-9895-d68be4176b12",
   "metadata": {},
   "source": [
    "## Train the PPO agent üèÉ\n",
    "- Let's train our agent for 1,000,000 timesteps, don't forget to use GPU on Colab. It will take approximately ~20min, but you can use fewer timesteps if you just want to try it out.\n",
    "- During the training, take a ‚òï break you deserved it ü§ó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63b995a1-7a8d-4c00-8448-2ee817947737",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 93       |\n",
      "|    ep_rew_mean     | -182     |\n",
      "| time/              |          |\n",
      "|    fps             | 2555     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 88.7        |\n",
      "|    ep_rew_mean          | -129        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1503        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008492629 |\n",
      "|    clip_fraction        | 0.0707      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.000622    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.73e+03    |\n",
      "|    n_updates            | 4           |\n",
      "|    policy_gradient_loss | -0.00595    |\n",
      "|    value_loss           | 4.96e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 93.3        |\n",
      "|    ep_rew_mean          | -122        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1356        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 36          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010114113 |\n",
      "|    clip_fraction        | 0.0542      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | -0.00596    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 371         |\n",
      "|    n_updates            | 8           |\n",
      "|    policy_gradient_loss | -0.00566    |\n",
      "|    value_loss           | 1.85e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 96.5        |\n",
      "|    ep_rew_mean          | -110        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1281        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 51          |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009197248 |\n",
      "|    clip_fraction        | 0.0459      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | -0.000899   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 307         |\n",
      "|    n_updates            | 12          |\n",
      "|    policy_gradient_loss | -0.00522    |\n",
      "|    value_loss           | 1.14e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 97.7        |\n",
      "|    ep_rew_mean          | -94.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1220        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 67          |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009662819 |\n",
      "|    clip_fraction        | 0.0946      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | -0.000202   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 150         |\n",
      "|    n_updates            | 16          |\n",
      "|    policy_gradient_loss | -0.00381    |\n",
      "|    value_loss           | 479         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 99.1        |\n",
      "|    ep_rew_mean          | -70.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1185        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 82          |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010251526 |\n",
      "|    clip_fraction        | 0.0914      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | -0.00171    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 267         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00861    |\n",
      "|    value_loss           | 413         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 105         |\n",
      "|    ep_rew_mean          | -53.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1153        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 99          |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010376143 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | -0.00014    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 143         |\n",
      "|    n_updates            | 24          |\n",
      "|    policy_gradient_loss | -0.00727    |\n",
      "|    value_loss           | 335         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 107          |\n",
      "|    ep_rew_mean          | -48.3        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1144         |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 114          |\n",
      "|    total_timesteps      | 131072       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0095490785 |\n",
      "|    clip_fraction        | 0.0662       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.26        |\n",
      "|    explained_variance   | -1.14e-05    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 131          |\n",
      "|    n_updates            | 28           |\n",
      "|    policy_gradient_loss | -0.00523     |\n",
      "|    value_loss           | 350          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 107         |\n",
      "|    ep_rew_mean          | -33.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1123        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 131         |\n",
      "|    total_timesteps      | 147456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006101409 |\n",
      "|    clip_fraction        | 0.0499      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | -6.2e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 141         |\n",
      "|    n_updates            | 32          |\n",
      "|    policy_gradient_loss | -0.00484    |\n",
      "|    value_loss           | 431         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 118         |\n",
      "|    ep_rew_mean          | -32.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1093        |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 149         |\n",
      "|    total_timesteps      | 163840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007198549 |\n",
      "|    clip_fraction        | 0.0403      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | -2.71e-05   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 153         |\n",
      "|    n_updates            | 36          |\n",
      "|    policy_gradient_loss | -0.00387    |\n",
      "|    value_loss           | 455         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 123        |\n",
      "|    ep_rew_mean          | -22        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1051       |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 171        |\n",
      "|    total_timesteps      | 180224     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01051082 |\n",
      "|    clip_fraction        | 0.0789     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.2       |\n",
      "|    explained_variance   | 5.96e-07   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 206        |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.00463   |\n",
      "|    value_loss           | 475        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 145        |\n",
      "|    ep_rew_mean          | -17.7      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1037       |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 189        |\n",
      "|    total_timesteps      | 196608     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00856236 |\n",
      "|    clip_fraction        | 0.0809     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.17      |\n",
      "|    explained_variance   | -1.19e-07  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 428        |\n",
      "|    n_updates            | 44         |\n",
      "|    policy_gradient_loss | -0.00338   |\n",
      "|    value_loss           | 572        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 171         |\n",
      "|    ep_rew_mean          | -19.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 988         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 215         |\n",
      "|    total_timesteps      | 212992      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008157362 |\n",
      "|    clip_fraction        | 0.0473      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | 1.91e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 307         |\n",
      "|    n_updates            | 48          |\n",
      "|    policy_gradient_loss | -0.00167    |\n",
      "|    value_loss           | 685         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 208          |\n",
      "|    ep_rew_mean          | -13.8        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 947          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 242          |\n",
      "|    total_timesteps      | 229376       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061622653 |\n",
      "|    clip_fraction        | 0.0513       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.16        |\n",
      "|    explained_variance   | 0.000171     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 213          |\n",
      "|    n_updates            | 52           |\n",
      "|    policy_gradient_loss | -0.00225     |\n",
      "|    value_loss           | 578          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 273         |\n",
      "|    ep_rew_mean          | -15.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 890         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 276         |\n",
      "|    total_timesteps      | 245760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009296761 |\n",
      "|    clip_fraction        | 0.0627      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | -0.00267    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 365         |\n",
      "|    n_updates            | 56          |\n",
      "|    policy_gradient_loss | -0.00228    |\n",
      "|    value_loss           | 682         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 345         |\n",
      "|    ep_rew_mean          | -9.24       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 847         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 309         |\n",
      "|    total_timesteps      | 262144      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004551697 |\n",
      "|    clip_fraction        | 0.0196      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | -0.0117     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 264         |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00193    |\n",
      "|    value_loss           | 616         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 385          |\n",
      "|    ep_rew_mean          | -4.42        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 804          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 346          |\n",
      "|    total_timesteps      | 278528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031716707 |\n",
      "|    clip_fraction        | 0.00806      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.14        |\n",
      "|    explained_variance   | 0.118        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 201          |\n",
      "|    n_updates            | 64           |\n",
      "|    policy_gradient_loss | -0.00142     |\n",
      "|    value_loss           | 473          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 435          |\n",
      "|    ep_rew_mean          | -4.9         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 769          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 383          |\n",
      "|    total_timesteps      | 294912       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042528147 |\n",
      "|    clip_fraction        | 0.0106       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.14        |\n",
      "|    explained_variance   | 0.291        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 212          |\n",
      "|    n_updates            | 68           |\n",
      "|    policy_gradient_loss | -0.000945    |\n",
      "|    value_loss           | 505          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 475         |\n",
      "|    ep_rew_mean          | 1.33        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 718         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 433         |\n",
      "|    total_timesteps      | 311296      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004592155 |\n",
      "|    clip_fraction        | 0.019       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.411       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 211         |\n",
      "|    n_updates            | 72          |\n",
      "|    policy_gradient_loss | -0.00088    |\n",
      "|    value_loss           | 335         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 507         |\n",
      "|    ep_rew_mean          | 3.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 695         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 471         |\n",
      "|    total_timesteps      | 327680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005845437 |\n",
      "|    clip_fraction        | 0.0432      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | 0.532       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 128         |\n",
      "|    n_updates            | 76          |\n",
      "|    policy_gradient_loss | -0.00243    |\n",
      "|    value_loss           | 291         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 493          |\n",
      "|    ep_rew_mean          | 8.87         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 675          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 509          |\n",
      "|    total_timesteps      | 344064       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045124693 |\n",
      "|    clip_fraction        | 0.039        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.12        |\n",
      "|    explained_variance   | 0.541        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 86           |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00354     |\n",
      "|    value_loss           | 287          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 517         |\n",
      "|    ep_rew_mean          | 15.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 659         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 546         |\n",
      "|    total_timesteps      | 360448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004803594 |\n",
      "|    clip_fraction        | 0.0371      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 0.638       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 98.2        |\n",
      "|    n_updates            | 84          |\n",
      "|    policy_gradient_loss | -0.00204    |\n",
      "|    value_loss           | 304         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 548          |\n",
      "|    ep_rew_mean          | 26.6         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 640          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 588          |\n",
      "|    total_timesteps      | 376832       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042140824 |\n",
      "|    clip_fraction        | 0.0262       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0.728        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 78.2         |\n",
      "|    n_updates            | 88           |\n",
      "|    policy_gradient_loss | -0.00109     |\n",
      "|    value_loss           | 168          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 598         |\n",
      "|    ep_rew_mean          | 40.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 627         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 626         |\n",
      "|    total_timesteps      | 393216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007388577 |\n",
      "|    clip_fraction        | 0.0552      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0.818       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 86.3        |\n",
      "|    n_updates            | 92          |\n",
      "|    policy_gradient_loss | -0.00257    |\n",
      "|    value_loss           | 122         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 663        |\n",
      "|    ep_rew_mean          | 49.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 614        |\n",
      "|    iterations           | 25         |\n",
      "|    time_elapsed         | 666        |\n",
      "|    total_timesteps      | 409600     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00394724 |\n",
      "|    clip_fraction        | 0.0368     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.12      |\n",
      "|    explained_variance   | 0.775      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 126        |\n",
      "|    n_updates            | 96         |\n",
      "|    policy_gradient_loss | -0.000988  |\n",
      "|    value_loss           | 142        |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 737          |\n",
      "|    ep_rew_mean          | 63.5         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 602          |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 707          |\n",
      "|    total_timesteps      | 425984       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068268087 |\n",
      "|    clip_fraction        | 0.0506       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 0.778        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 92.3         |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00221     |\n",
      "|    value_loss           | 142          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 762          |\n",
      "|    ep_rew_mean          | 67.4         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 592          |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 746          |\n",
      "|    total_timesteps      | 442368       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032041194 |\n",
      "|    clip_fraction        | 0.0396       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 36.5         |\n",
      "|    n_updates            | 104          |\n",
      "|    policy_gradient_loss | -0.00167     |\n",
      "|    value_loss           | 67.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 824          |\n",
      "|    ep_rew_mean          | 79.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 581          |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 788          |\n",
      "|    total_timesteps      | 458752       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048295744 |\n",
      "|    clip_fraction        | 0.0218       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.925        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 20.8         |\n",
      "|    n_updates            | 108          |\n",
      "|    policy_gradient_loss | -0.00084     |\n",
      "|    value_loss           | 55.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 875         |\n",
      "|    ep_rew_mean          | 90.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 568         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 836         |\n",
      "|    total_timesteps      | 475136      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005205146 |\n",
      "|    clip_fraction        | 0.0443      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.95        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23.2        |\n",
      "|    n_updates            | 112         |\n",
      "|    policy_gradient_loss | -0.00143    |\n",
      "|    value_loss           | 30          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 901         |\n",
      "|    ep_rew_mean          | 102         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 560         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 876         |\n",
      "|    total_timesteps      | 491520      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003638148 |\n",
      "|    clip_fraction        | 0.0255      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.45        |\n",
      "|    n_updates            | 116         |\n",
      "|    policy_gradient_loss | -0.000502   |\n",
      "|    value_loss           | 15.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 935          |\n",
      "|    ep_rew_mean          | 112          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 550          |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 922          |\n",
      "|    total_timesteps      | 507904       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050551374 |\n",
      "|    clip_fraction        | 0.0395       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0.959        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.86         |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00243     |\n",
      "|    value_loss           | 24.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 942         |\n",
      "|    ep_rew_mean          | 116         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 540         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 969         |\n",
      "|    total_timesteps      | 524288      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006720935 |\n",
      "|    clip_fraction        | 0.0679      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.954       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.9        |\n",
      "|    n_updates            | 124         |\n",
      "|    policy_gradient_loss | -0.00134    |\n",
      "|    value_loss           | 31.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 964         |\n",
      "|    ep_rew_mean          | 121         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 534         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 1012        |\n",
      "|    total_timesteps      | 540672      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005623389 |\n",
      "|    clip_fraction        | 0.0277      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.993      |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.25        |\n",
      "|    n_updates            | 128         |\n",
      "|    policy_gradient_loss | -0.000909   |\n",
      "|    value_loss           | 27          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 965         |\n",
      "|    ep_rew_mean          | 125         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 529         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 1052        |\n",
      "|    total_timesteps      | 557056      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005171701 |\n",
      "|    clip_fraction        | 0.0301      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.965      |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.42        |\n",
      "|    n_updates            | 132         |\n",
      "|    policy_gradient_loss | -0.000408   |\n",
      "|    value_loss           | 25.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 965         |\n",
      "|    ep_rew_mean          | 129         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 524         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 1093        |\n",
      "|    total_timesteps      | 573440      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005000794 |\n",
      "|    clip_fraction        | 0.0298      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.982      |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.2         |\n",
      "|    n_updates            | 136         |\n",
      "|    policy_gradient_loss | -0.000326   |\n",
      "|    value_loss           | 19.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 972          |\n",
      "|    ep_rew_mean          | 134          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 520          |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 1134         |\n",
      "|    total_timesteps      | 589824       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043093967 |\n",
      "|    clip_fraction        | 0.0263       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.984       |\n",
      "|    explained_variance   | 0.989        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.48         |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -2.83e-06    |\n",
      "|    value_loss           | 9.78         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 979         |\n",
      "|    ep_rew_mean          | 138         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 513         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 1180        |\n",
      "|    total_timesteps      | 606208      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006575873 |\n",
      "|    clip_fraction        | 0.0279      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.952      |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.24        |\n",
      "|    n_updates            | 144         |\n",
      "|    policy_gradient_loss | -0.00102    |\n",
      "|    value_loss           | 11.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 986         |\n",
      "|    ep_rew_mean          | 142         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 510         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 1220        |\n",
      "|    total_timesteps      | 622592      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005417189 |\n",
      "|    clip_fraction        | 0.0249      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.907      |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.99        |\n",
      "|    n_updates            | 148         |\n",
      "|    policy_gradient_loss | -0.000775   |\n",
      "|    value_loss           | 6.84        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 985         |\n",
      "|    ep_rew_mean          | 148         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 507         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 1259        |\n",
      "|    total_timesteps      | 638976      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004806166 |\n",
      "|    clip_fraction        | 0.0387      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.837      |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.65        |\n",
      "|    n_updates            | 152         |\n",
      "|    policy_gradient_loss | -0.00134    |\n",
      "|    value_loss           | 7.66        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 977          |\n",
      "|    ep_rew_mean          | 154          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 504          |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 1298         |\n",
      "|    total_timesteps      | 655360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031735883 |\n",
      "|    clip_fraction        | 0.0198       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.828       |\n",
      "|    explained_variance   | 0.963        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.09         |\n",
      "|    n_updates            | 156          |\n",
      "|    policy_gradient_loss | -0.000546    |\n",
      "|    value_loss           | 48.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 977          |\n",
      "|    ep_rew_mean          | 153          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 501          |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 1339         |\n",
      "|    total_timesteps      | 671744       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046179863 |\n",
      "|    clip_fraction        | 0.0508       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.758       |\n",
      "|    explained_variance   | 0.951        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.82         |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.00118     |\n",
      "|    value_loss           | 66.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 965         |\n",
      "|    ep_rew_mean          | 158         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 492         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 1395        |\n",
      "|    total_timesteps      | 688128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003012448 |\n",
      "|    clip_fraction        | 0.0436      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.793      |\n",
      "|    explained_variance   | 0.996       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.51        |\n",
      "|    n_updates            | 164         |\n",
      "|    policy_gradient_loss | -0.000202   |\n",
      "|    value_loss           | 4.3         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 956          |\n",
      "|    ep_rew_mean          | 163          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 483          |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 1456         |\n",
      "|    total_timesteps      | 704512       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030523185 |\n",
      "|    clip_fraction        | 0.0324       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.769       |\n",
      "|    explained_variance   | 0.932        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 43.8         |\n",
      "|    n_updates            | 168          |\n",
      "|    policy_gradient_loss | -0.000573    |\n",
      "|    value_loss           | 101          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 888          |\n",
      "|    ep_rew_mean          | 166          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 477          |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 1510         |\n",
      "|    total_timesteps      | 720896       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058512297 |\n",
      "|    clip_fraction        | 0.0645       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.748       |\n",
      "|    explained_variance   | 0.905        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.86         |\n",
      "|    n_updates            | 172          |\n",
      "|    policy_gradient_loss | -0.00293     |\n",
      "|    value_loss           | 120          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 760          |\n",
      "|    ep_rew_mean          | 191          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 474          |\n",
      "|    iterations           | 45           |\n",
      "|    time_elapsed         | 1554         |\n",
      "|    total_timesteps      | 737280       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057969536 |\n",
      "|    clip_fraction        | 0.0647       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.756       |\n",
      "|    explained_variance   | 0.83         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 74           |\n",
      "|    n_updates            | 176          |\n",
      "|    policy_gradient_loss | -0.00233     |\n",
      "|    value_loss           | 237          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 547          |\n",
      "|    ep_rew_mean          | 218          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 472          |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 1593         |\n",
      "|    total_timesteps      | 753664       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0103096375 |\n",
      "|    clip_fraction        | 0.0871       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.754       |\n",
      "|    explained_variance   | 0.763        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 53.9         |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00467     |\n",
      "|    value_loss           | 312          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 419         |\n",
      "|    ep_rew_mean          | 232         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 470         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 1638        |\n",
      "|    total_timesteps      | 770048      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005355965 |\n",
      "|    clip_fraction        | 0.0355      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.784      |\n",
      "|    explained_variance   | 0.681       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 67.7        |\n",
      "|    n_updates            | 184         |\n",
      "|    policy_gradient_loss | -0.00139    |\n",
      "|    value_loss           | 262         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 365         |\n",
      "|    ep_rew_mean          | 234         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 467         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 1681        |\n",
      "|    total_timesteps      | 786432      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004244863 |\n",
      "|    clip_fraction        | 0.0374      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.776      |\n",
      "|    explained_variance   | 0.748       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 267         |\n",
      "|    n_updates            | 188         |\n",
      "|    policy_gradient_loss | -0.00114    |\n",
      "|    value_loss           | 329         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 364          |\n",
      "|    ep_rew_mean          | 243          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 466          |\n",
      "|    iterations           | 49           |\n",
      "|    time_elapsed         | 1720         |\n",
      "|    total_timesteps      | 802816       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053652087 |\n",
      "|    clip_fraction        | 0.0533       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.803       |\n",
      "|    explained_variance   | 0.722        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 228          |\n",
      "|    n_updates            | 192          |\n",
      "|    policy_gradient_loss | -0.00145     |\n",
      "|    value_loss           | 252          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 348         |\n",
      "|    ep_rew_mean          | 242         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 469         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 1745        |\n",
      "|    total_timesteps      | 819200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002914764 |\n",
      "|    clip_fraction        | 0.0418      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.753      |\n",
      "|    explained_variance   | 0.78        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 155         |\n",
      "|    n_updates            | 196         |\n",
      "|    policy_gradient_loss | -7.48e-05   |\n",
      "|    value_loss           | 242         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 343          |\n",
      "|    ep_rew_mean          | 246          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 472          |\n",
      "|    iterations           | 51           |\n",
      "|    time_elapsed         | 1769         |\n",
      "|    total_timesteps      | 835584       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026227273 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.753       |\n",
      "|    explained_variance   | 0.776        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 171          |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.000355    |\n",
      "|    value_loss           | 352          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 350         |\n",
      "|    ep_rew_mean          | 259         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 475         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 1790        |\n",
      "|    total_timesteps      | 851968      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003188409 |\n",
      "|    clip_fraction        | 0.0269      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.74       |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 59.9        |\n",
      "|    n_updates            | 204         |\n",
      "|    policy_gradient_loss | -3.66e-05   |\n",
      "|    value_loss           | 136         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 334          |\n",
      "|    ep_rew_mean          | 258          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 479          |\n",
      "|    iterations           | 53           |\n",
      "|    time_elapsed         | 1811         |\n",
      "|    total_timesteps      | 868352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031829313 |\n",
      "|    clip_fraction        | 0.03         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.739       |\n",
      "|    explained_variance   | 0.946        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.3         |\n",
      "|    n_updates            | 208          |\n",
      "|    policy_gradient_loss | -0.000265    |\n",
      "|    value_loss           | 50.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 308         |\n",
      "|    ep_rew_mean          | 244         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 482         |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 1833        |\n",
      "|    total_timesteps      | 884736      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004499769 |\n",
      "|    clip_fraction        | 0.0312      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.733      |\n",
      "|    explained_variance   | 0.846       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 73          |\n",
      "|    n_updates            | 212         |\n",
      "|    policy_gradient_loss | -0.000998   |\n",
      "|    value_loss           | 158         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 307         |\n",
      "|    ep_rew_mean          | 252         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 485         |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 1857        |\n",
      "|    total_timesteps      | 901120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002772269 |\n",
      "|    clip_fraction        | 0.0128      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.73       |\n",
      "|    explained_variance   | 0.776       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 116         |\n",
      "|    n_updates            | 216         |\n",
      "|    policy_gradient_loss | -0.00133    |\n",
      "|    value_loss           | 428         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 298          |\n",
      "|    ep_rew_mean          | 265          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 488          |\n",
      "|    iterations           | 56           |\n",
      "|    time_elapsed         | 1876         |\n",
      "|    total_timesteps      | 917504       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032429465 |\n",
      "|    clip_fraction        | 0.0296       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.728       |\n",
      "|    explained_variance   | 0.875        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 42.7         |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.000538    |\n",
      "|    value_loss           | 212          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 301          |\n",
      "|    ep_rew_mean          | 258          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 492          |\n",
      "|    iterations           | 57           |\n",
      "|    time_elapsed         | 1895         |\n",
      "|    total_timesteps      | 933888       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043725395 |\n",
      "|    clip_fraction        | 0.0396       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.752       |\n",
      "|    explained_variance   | 0.957        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 26.2         |\n",
      "|    n_updates            | 224          |\n",
      "|    policy_gradient_loss | -0.000245    |\n",
      "|    value_loss           | 65.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 306          |\n",
      "|    ep_rew_mean          | 257          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 495          |\n",
      "|    iterations           | 58           |\n",
      "|    time_elapsed         | 1918         |\n",
      "|    total_timesteps      | 950272       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037297807 |\n",
      "|    clip_fraction        | 0.0337       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.695       |\n",
      "|    explained_variance   | 0.87         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 205          |\n",
      "|    n_updates            | 228          |\n",
      "|    policy_gradient_loss | -0.0017      |\n",
      "|    value_loss           | 225          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 315         |\n",
      "|    ep_rew_mean          | 263         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 497         |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 1942        |\n",
      "|    total_timesteps      | 966656      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002306088 |\n",
      "|    clip_fraction        | 0.0233      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.729      |\n",
      "|    explained_variance   | 0.964       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 34.2        |\n",
      "|    n_updates            | 232         |\n",
      "|    policy_gradient_loss | -0.000668   |\n",
      "|    value_loss           | 72.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 319         |\n",
      "|    ep_rew_mean          | 262         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 499         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 1966        |\n",
      "|    total_timesteps      | 983040      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004757966 |\n",
      "|    clip_fraction        | 0.0396      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.757      |\n",
      "|    explained_variance   | 0.961       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 256         |\n",
      "|    n_updates            | 236         |\n",
      "|    policy_gradient_loss | 3.44e-05    |\n",
      "|    value_loss           | 87          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 295         |\n",
      "|    ep_rew_mean          | 266         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 503         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 1984        |\n",
      "|    total_timesteps      | 999424      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003888352 |\n",
      "|    clip_fraction        | 0.042       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.706      |\n",
      "|    explained_variance   | 0.96        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26.6        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.000666   |\n",
      "|    value_loss           | 61          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 294         |\n",
      "|    ep_rew_mean          | 264         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 506         |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 2003        |\n",
      "|    total_timesteps      | 1015808     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004849784 |\n",
      "|    clip_fraction        | 0.0496      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.776      |\n",
      "|    explained_variance   | 0.958       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 265         |\n",
      "|    n_updates            | 244         |\n",
      "|    policy_gradient_loss | -0.000626   |\n",
      "|    value_loss           | 69.6        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\save_util.py:284: UserWarning: Path 'model' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"
     ]
    }
   ],
   "source": [
    "# Train it for 1,000,000 timesteps\n",
    "model.learn(total_timesteps=1000000)\n",
    "# Save the model\n",
    "model_name = \"model/ppo-LunarLander-v2\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605947f1-0d33-477d-bf15-290170eccb40",
   "metadata": {},
   "source": [
    "## Evaluate the agent üìà\n",
    "- Remember to wrap the environment in a [Monitor](https://stable-baselines3.readthedocs.io/en/master/common/monitor.html).\n",
    "- Now that our Lunar Lander agent is trained üöÄ, we need to **check its performance**.\n",
    "- Stable-Baselines3 provides a method to do that: `evaluate_policy`.\n",
    "- To fill that part you need to [check the documentation](https://stable-baselines3.readthedocs.io/en/master/guide/examples.html#basic-usage-training-saving-loading)\n",
    "- In the next step,  we'll see **how to automatically evaluate and share your agent to compete in a leaderboard, but for now let's do it ourselves**\n",
    "\n",
    "\n",
    "üí° When you evaluate your agent, you should not use your training environment but create an evaluation environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b805705-78cb-415e-ac67-40a3029a1fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward=266.60 +/- 16.72747123461866\n"
     ]
    }
   ],
   "source": [
    "# Create a new environment for evaluation\n",
    "eval_env = Monitor(gym.make(\"LunarLander-v2\", render_mode='rgb_array'))\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457bde14-fd16-48e0-852a-546f08422c02",
   "metadata": {},
   "source": [
    "- In my case, I got a mean reward of `200.20 +/- 20.80` after training for 1 million steps, which means that our lunar lander agent is ready to land on the moon üåõü•≥."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d1de191-ae51-46d0-9dff-b8819ce0d993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "789975e96d11459f83f4d06289dcf788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()\n",
    "!git config --global credential.helper store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cc78f8-15fc-40c6-9d61-4e9051e9c9ea",
   "metadata": {},
   "source": [
    "If you don't want to use a Google Colab or a Jupyter Notebook, you need to use this command instead: `huggingface-cli login`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d311db-f551-4899-be34-6cf190b658bc",
   "metadata": {},
   "source": [
    "3Ô∏è‚É£ We're now ready to push our trained agent to the ü§ó Hub üî• using `package_to_hub()` function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2f91d7-3e55-4fa9-9cd4-e38502802607",
   "metadata": {},
   "source": [
    "Let's fill the `package_to_hub` function:\n",
    "- `model`: our trained model.\n",
    "- `model_name`: the name of the trained model that we defined in `model_save`\n",
    "- `model_architecture`: the model architecture we used, in our case PPO\n",
    "- `env_id`: the name of the environment, in our case `LunarLander-v2`\n",
    "- `eval_env`: the evaluation environment defined in eval_env\n",
    "- `repo_id`: the name of the Hugging Face Hub Repository that will be created/updated `(repo_id = {username}/{repo_name})`\n",
    "\n",
    "üí° **A good name is {username}/{model_architecture}-{env_id}**\n",
    "\n",
    "- `commit_message`: message of the commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f39b86d-cdd2-427c-ad24-df64c7a6d433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4m‚Ñπ This function will save, evaluate, generate a video of your agent,\n",
      "create a model card and push everything to the hub. It might take up to 1min.\n",
      "This is a work in progress: if you encounter a bug, please open an issue.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\save_util.py:284: UserWarning: Path 'C:\\Users\\hp\\AppData\\Local\\Temp\\tmpjtxvsaob\\model' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving video to C:\\Users\\hp\\AppData\\Local\\Temp\\tmpt_a9xfac\\-step-0-to-step-1000.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function VecVideoRecorder.__del__ at 0x000002393685BC70>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hp\\anaconda3\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\vec_video_recorder.py\", line 113, in __del__\n",
      "    self.close_video_recorder()\n",
      "  File \"C:\\Users\\hp\\anaconda3\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\vec_video_recorder.py\", line 104, in close_video_recorder\n",
      "    self.video_recorder.close()\n",
      "  File \"C:\\Users\\hp\\anaconda3\\envs\\RL\\lib\\site-packages\\gymnasium\\wrappers\\monitoring\\video_recorder.py\", line 157, in close\n",
      "    clip.write_videofile(self.path, logger=moviepy_logger)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\envs\\RL\\lib\\site-packages\\decorator.py\", line 232, in fun\n",
      "    for i, extra in enumerate(extras):\n",
      "  File \"C:\\Users\\hp\\anaconda3\\envs\\RL\\lib\\site-packages\\moviepy\\decorators.py\", line 54, in requires_duration\n",
      "    return f(clip, *a, **k)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\envs\\RL\\lib\\site-packages\\decorator.py\", line 232, in fun\n",
      "    for i, extra in enumerate(extras):\n",
      "  File \"C:\\Users\\hp\\anaconda3\\envs\\RL\\lib\\site-packages\\moviepy\\decorators.py\", line 135, in use_clip_fps_by_default\n",
      "    return f(clip, *new_a, **new_kw)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\envs\\RL\\lib\\site-packages\\decorator.py\", line 232, in fun\n",
      "    for i, extra in enumerate(extras):\n",
      "  File \"C:\\Users\\hp\\anaconda3\\envs\\RL\\lib\\site-packages\\moviepy\\decorators.py\", line 22, in convert_masks_to_RGB\n",
      "    return f(clip, *a, **k)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\envs\\RL\\lib\\site-packages\\moviepy\\video\\VideoClip.py\", line 300, in write_videofile\n",
      "    ffmpeg_write_video(self, filename, fps, codec,\n",
      "  File \"C:\\Users\\hp\\anaconda3\\envs\\RL\\lib\\site-packages\\moviepy\\video\\io\\ffmpeg_writer.py\", line 213, in ffmpeg_write_video\n",
      "    with FFMPEG_VideoWriter(filename, clip.size, fps, codec = codec,\n",
      "  File \"C:\\Users\\hp\\anaconda3\\envs\\RL\\lib\\site-packages\\moviepy\\video\\io\\ffmpeg_writer.py\", line 88, in __init__\n",
      "    '-r', '%.02f' % fps,\n",
      "TypeError: must be real number, not NoneType\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video C:\\Users\\hp\\AppData\\Local\\Temp\\tmpt_a9xfac\\-step-0-to-step-1000.mp4.\n",
      "Moviepy - Writing video C:\\Users\\hp\\AppData\\Local\\Temp\\tmpt_a9xfac\\-step-0-to-step-1000.mp4\n",
      "\n",
      "\u001b[38;5;1m‚úò must be real number, not NoneType\u001b[0m\n",
      "\u001b[38;5;1m‚úò We are unable to generate a replay of your agent, the package_to_hub\n",
      "process continues\u001b[0m\n",
      "\u001b[38;5;1m‚úò Please open an issue at\n",
      "https://github.com/huggingface/huggingface_sb3/issues\u001b[0m\n",
      "Moviepy - Building video C:\\Users\\hp\\AppData\\Local\\Temp\\tmpt_a9xfac\\-step-0-to-step-1000.mp4.\n",
      "Moviepy - Writing video C:\\Users\\hp\\AppData\\Local\\Temp\\tmpt_a9xfac\\-step-0-to-step-1000.mp4\n",
      "\n",
      "\u001b[38;5;4m‚Ñπ Pushing repo NiloyKumarKundu/ppo-LunarLander-v2 to the Hugging Face\n",
      "Hub\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\envs\\RL\\lib\\site-packages\\gymnasium\\wrappers\\monitoring\\video_recorder.py:178: UserWarning: \u001b[33mWARN: Unable to save last video! Did you call close()?\u001b[0m\n",
      "  logger.warn(\"Unable to save last video! Did you call close()?\")\n",
      "policy.optimizer.pth:   0%|                                                                | 0.00/88.0k [00:00<?, ?B/s]\n",
      "policy.pth:   0%|                                                                          | 0.00/43.6k [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "pytorch_variables.pth:   0%|                                                                 | 0.00/864 [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Upload 4 LFS files:   0%|                                                                        | 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "policy.optimizer.pth:  19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                             | 16.4k/88.0k [00:00<00:00, 120kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_variables.pth: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 864/864 [00:00<00:00, 8.00kB/s]\u001b[A\u001b[A\n",
      "policy.pth:  38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                         | 16.4k/43.6k [00:00<00:00, 132kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "policy.pth: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43.6k/43.6k [00:00<00:00, 67.6kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "pytorch_variables.pth: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 864/864 [00:01<00:00, 815B/s]\n",
      "policy.optimizer.pth: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 88.0k/88.0k [00:01<00:00, 61.8kB/s]\n",
      "ppo-LunarLander-v2.zip: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150k/150k [00:01<00:00, 95.0kB/s]\n",
      "\n",
      "\n",
      "\n",
      "Upload 4 LFS files:  25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                | 1/4 [00:02<00:06,  2.14s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Upload 4 LFS files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.73it/s]\u001b[A\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4m‚Ñπ Your model is pushed to the Hub. You can view your model here:\n",
      "https://huggingface.co/NiloyKumarKundu/ppo-LunarLander-v2/tree/main/\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/NiloyKumarKundu/ppo-LunarLander-v2/commit/7a826a326eac981c07ef16f953fed23abe6e7ecf', commit_message='Upload PPO LunarLander-v2 trained agent', commit_description='', oid='7a826a326eac981c07ef16f953fed23abe6e7ecf', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "from huggingface_sb3 import package_to_hub\n",
    "\n",
    "# PLACE the variables you've just defined two cells above\n",
    "# Define the name of the environment\n",
    "env_id = \"LunarLander-v2\"\n",
    "\n",
    "# TODO: Define the model architecture we used\n",
    "model_architecture = \"PPO\"\n",
    "\n",
    "## Define a repo_id\n",
    "## repo_id is the id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n",
    "## CHANGE WITH YOUR REPO ID\n",
    "repo_id = \"NiloyKumarKundu/ppo-LunarLander-v2\" # Change with your repo id, you can't push with mine üòÑ\n",
    "\n",
    "## Define the commit message\n",
    "commit_message = \"Upload PPO LunarLander-v2 trained agent\"\n",
    "\n",
    "# Create the evaluation env and set the render_mode=\"rgb_array\"\n",
    "eval_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n",
    "\n",
    "# PLACE the package_to_hub function you've just filled here\n",
    "package_to_hub(model=model, # Our trained model\n",
    "               model_name=model_name, # The name of our trained model\n",
    "               model_architecture=model_architecture, # The model architecture we used: in our case PPO\n",
    "               env_id=env_id, # Name of the environment\n",
    "               eval_env=eval_env, # Evaluation Environment\n",
    "               repo_id=repo_id, # id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n",
    "               commit_message=commit_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23347c59-fc1d-4182-9168-8945e8eee912",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RL] *",
   "language": "python",
   "name": "conda-env-RL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
