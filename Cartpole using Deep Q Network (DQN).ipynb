{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1643b587-5469-451d-b7ba-03e580f6191c",
   "metadata": {},
   "source": [
    "We'll use the following from PyTorch:\n",
    "\n",
    "- neural networks(torch.nn)\n",
    "This module provides tools for building neural networks. It includes a wide range of layer types, such as fully-connected layers, convolutional layers, and recurrent layers, as well as activation functions and loss functions.\n",
    "\n",
    "\n",
    "- Optimization (torch.optim)\n",
    "This module provides a range of optimization algorithms for training neural networks. It includes classic optimization algorithms such as Stochastic Gradient Descent (SGD), as well as more advanced algorithms like Adam and RMSProp.\n",
    "\n",
    "- automatic differentitation (torch.autograd)\n",
    "This module provides automatic differentiation functionality, which is essential for training neural networks via backpropagation. It enables PyTorch to automatically compute gradients of a loss fuction with respect to all the parameters of the network, allowing optimization algorithms to adjust the parameters in order to minimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "562a5df5-2313-4092-bae8-60412cc4ad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be141f0e-4faa-47ac-a2e1-7911b2f65c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74cd5674-92b5-452c-be96-ec9e06c5133d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x22e89d912d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#is_ipython = 'inline' in matplotlib.get_backend\n",
    "\n",
    "#if is_python:\n",
    "    #from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19b33687-e090-4c20-a005-0c50b4c265e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eefd7c-db8b-4eb3-8502-b948b0ea1ddd",
   "metadata": {},
   "source": [
    "# Replay Memory\n",
    "\n",
    "1. Replay Memory is a technique used in reinforcement learning to store and manage the experiences of an agent during training. The idea is to store the agent's experiences as a sequence of (state, action, reward, next_state) tuples, which are collected as the agent interacts with the environment. During training, these experiments are used to update the agent's policy and value function.\n",
    "\n",
    "2. The Replay Memory allows the agent to learn from past experiences by randomly sampling a batch of experiences from the memory buffer, rather than just learning from the most recent experience. This helps to reduce the correlation between subsequent experiences, which can improve the stability and convergence of the learning algorithm. In addition, by storing experiences in a buffer, the agent can re-use past experiences to update it's policy and value function multiple times, which can further improve learning efficiency.\n",
    "\n",
    "3. The Replay Memory is typically implemented as a fixed-size buffer or queue that stores the most recent experiences. When the buffer is full, new experiences overwrite the oldest experiences in the bufer. During training, a batch of experiences is randomly sampled from the buffer and used to update the agent's policy and value function. This process is repeated iteratively untill the agent converges to an optimal policy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74412c47-3496-4aa3-8a5c-7a0f8c2b6d29",
   "metadata": {},
   "source": [
    "# How to use the concept of Replay Memory to implement DQN algorithm\n",
    "\n",
    "We'll be using experience replay memory for training our DQN. It stores the transition that the agent observes, allowing us to reuse this data later. By sampling from it randomly, the trasitions that build up a batch are decorrelated. It has been shown that this greatly stabilizes and improves the DQN training procedure. \n",
    "\n",
    "For this, we're going to need two classes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e775c7-7696-4e8f-adff-2c6a1eab8ef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f594d3-af68-45dc-bd11-c27da4c24cb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d23fc6a-ded6-4bcd-a3ff-cc26506ffee8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RL] *",
   "language": "python",
   "name": "conda-env-RL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
