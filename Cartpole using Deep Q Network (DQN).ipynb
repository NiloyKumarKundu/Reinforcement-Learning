{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1643b587-5469-451d-b7ba-03e580f6191c",
   "metadata": {},
   "source": [
    "We'll use the following from PyTorch:\n",
    "\n",
    "- neural networks(torch.nn)\n",
    "This module provides tools for building neural networks. It includes a wide range of layer types, such as fully-connected layers, convolutional layers, and recurrent layers, as well as activation functions and loss functions.\n",
    "\n",
    "\n",
    "- Optimization (torch.optim)\n",
    "This module provides a range of optimization algorithms for training neural networks. It includes classic optimization algorithms such as Stochastic Gradient Descent (SGD), as well as more advanced algorithms like Adam and RMSProp.\n",
    "\n",
    "- automatic differentitation (torch.autograd)\n",
    "This module provides automatic differentiation functionality, which is essential for training neural networks via backpropagation. It enables PyTorch to automatically compute gradients of a loss fuction with respect to all the parameters of the network, allowing optimization algorithms to adjust the parameters in order to minimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "562a5df5-2313-4092-bae8-60412cc4ad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be141f0e-4faa-47ac-a2e1-7911b2f65c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74cd5674-92b5-452c-be96-ec9e06c5133d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x18987d2c250>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19b33687-e090-4c20-a005-0c50b4c265e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eefd7c-db8b-4eb3-8502-b948b0ea1ddd",
   "metadata": {},
   "source": [
    "# Replay Memory\n",
    "\n",
    "1. Replay Memory is a technique used in reinforcement learning to store and manage the experiences of an agent during training. The idea is to store the agent's experiences as a sequence of (state, action, reward, next_state) tuples, which are collected as the agent interacts with the environment. During training, these experiments are used to update the agent's policy and value function.\n",
    "\n",
    "2. The Replay Memory allows the agent to learn from past experiences by randomly sampling a batch of experiences from the memory buffer, rather than just learning from the most recent experience. This helps to reduce the correlation between subsequent experiences, which can improve the stability and convergence of the learning algorithm. In addition, by storing experiences in a buffer, the agent can re-use past experiences to update it's policy and value function multiple times, which can further improve learning efficiency.\n",
    "\n",
    "3. The Replay Memory is typically implemented as a fixed-size buffer or queue that stores the most recent experiences. When the buffer is full, new experiences overwrite the oldest experiences in the bufer. During training, a batch of experiences is randomly sampled from the buffer and used to update the agent's policy and value function. This process is repeated iteratively untill the agent converges to an optimal policy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74412c47-3496-4aa3-8a5c-7a0f8c2b6d29",
   "metadata": {},
   "source": [
    "# How to use the concept of Replay Memory to implement DQN algorithm\n",
    "\n",
    "We'll be using experience replay memory for training our DQN. It stores the transition that the agent observes, allowing us to reuse this data later. By sampling from it randomly, the trasitions that build up a batch are decorrelated. It has been shown that this greatly stabilizes and improves the DQN training procedure. \n",
    "\n",
    "For this, we're going to need two classes:\n",
    "1. Transition - a named tuple representing a single transition in our environment. It essentially maps (state, action) pairs to their (next_state, reward) result, with being the screen difference image as described later on.\n",
    "\n",
    "2. ReplayMemory - a cyclic buffer of bounded size that holds the transitions observe recently. It also implements a .sample() method for selecting a random batch of transitions for training. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7e775c7-7696-4e8f-adff-2c6a1eab8ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41f594d3-af68-45dc-bd11-c27da4c24cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args): # take states\n",
    "        \"\"\"\n",
    "        Save a transition\n",
    "        \"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c8b53d-e494-45ef-be90-8da1fdaa8590",
   "metadata": {},
   "source": [
    "# DQN Algorithm\n",
    "\n",
    "DQN (Deep Q-Network) is a reinforcement learning algorithm that uses deep neural networks to approximate the Q-function in a Q-learning algorithm.\n",
    "\n",
    "The DQN algorithm in the context of the Cartpole environment can be summarized following steps:\n",
    "1. Initialize the Q-network with random weights.\n",
    "2. Sample an action using an epsilon-greedy policy, which selects the action with the highest Q-value with probability 1-epsilon and a random action with probability epsilon.\n",
    "3. Execute the action and observe the next state and reward.\n",
    "4. Store the experience tuple (state, action, reward, next_state) in a reply buffer.\n",
    "5. Sample a mini-batch of experiences from the replay buffer.\n",
    "6. Compute the Q-target values for the mini-batch using the Bellman equation: Q_t = reward + gamma * max_a(Q(next_state, a))\n",
    "7. Compute the Q-values for the mini-batch using the current Q-network: Q_values Q(state, action).\n",
    "8. Compute the loss between the Q-values and the Q-target values and update the network parameters using gradient descent.\n",
    "9. Repeat steps 2-8 for a fixed number of episodes or untill convergence.\n",
    "\n",
    "The DQN algorithm uses a target network to stabilize the training process. The target network is a copy of the Q-network that is updated less frequently than the Q-network. This helps to prevent the Q-values from oscillating during training.\n",
    "\n",
    "In the Cartpole environment, the DQN algorithm learns to balance the pole on the cart moving the cart left or right. The Q-network takes the state of the environment as inputs and outputs the Q-values for each possible action. The DQN algorithm learns to maximize the Q-values by updating the Q-network parameters using gradient descent. With enough training, the DQN algorithm can learn to balance the pole on the cart extended periods of time.\n",
    "\n",
    "Our model will be a convolutional neural netwrok that takes in the difference between the current and previous screen patches. It has two outputs, representing `Q(s, left)`, `Q(s, right)` (where s is the input to the network). In effect, the network is trying to predict the expected return of taking each action given the current input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f7b0289-c366-475d-949e-83141b14f6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module): \n",
    "    # Multi-layer perceptron with three layers\n",
    "    # n_observations is our input (state of the environment) to the network\n",
    "    # n_action - number of possible actions in the environment\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    # take input and pass to 3 layer of the neural network\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f61d42c-dc47-4b7f-ba2c-134048bc7a7d",
   "metadata": {},
   "source": [
    "# Training\n",
    "During training, the Q-network is updated using the Bellman equation to minimize the mean squared error between the predicted Q-values and the target Q-values. The target Q-values are computed using the Q-network, but with the weights frozen and not updated during the current iteration of training. This helps to stabilize the training process and prevent the network from overfitting to the training data.\n",
    "\n",
    "`select_action` - will select an action accordingly to an epsilon greedy policy. Simply we'll sometimes use our model for choosing the action, and sometimes we'll just sample one uniformly. The probability of choosing a random action will start at `EPS_START` and will decay exponentially towards `EPS_END`. `EPS_DECAY` controls the rate of the decay.\n",
    "\n",
    "`plot_durations` - a helper for plotting the durations of episodes, along with an average over the last 100 episodes (the measure used in the official evaluations). The plot underneath the cell containing the main training loop, and will update after every episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9debcfe8-8fc9-4ec3-a487-967bf650e16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of transitions smaples from the replay buffer\n",
    "BATCH_SIZE = 128\n",
    "# GAMMA is the discount factor\n",
    "GAMMA = 0.99\n",
    "# EPS_START is the starting value of epsilon\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05 # Ending value of epsilon\n",
    "EPS_DECAY = 1000 # controls the rate of exponential decay of epsilon; \n",
    "# higher means slower decay\n",
    "TAU = 0.005 # update rate of the target network\n",
    "LR = 1e-4 # learning rate of AdamW optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fbbae6-a45a-4da1-b721-91e861618700",
   "metadata": {},
   "source": [
    "# Adam Optimizer\n",
    "\n",
    "Adam (Adaptive Moment Estimation) is a popular optimization algorithm that is commonly used in deep learning. It is an extension of stochastic gradient descent (SGD), which is the most basic optimization algorithm used to train neural network. The main idea behind Adam is to combine the advantages of two other optimization techniques, AdaGrad and RMSProp.\n",
    "\n",
    "In the DQN algorithm, we use the Adam optimizer to update the weights of our neural network based on the gradients of the loss function with respect to the parameters. Specifically, we use the AdamW optimizer, which is a variant of Adam that also incorporates weight decay regularization. Weight decay helps prevent overfitting by adding a penalty to the loss function that is proportional to the magnitude of the weights. By adding this penalty, the optimizer encourages the network to learn simpler and more generalizable representations.\n",
    "\n",
    "The learning rate (LR) is a hyperparameter that controls the step size taken during the optimization. It is an important parameter to tune, as a high learning rate can cause optimizer to overshoot the optimal weights and lead to divergence, while a low learning rate can result in slow convergence and getting stuck in local minima. In the DQN algorithm, we set the learning rate to 1e-4.\n",
    "\n",
    "In summary, the AdamW optimizer is a widely used optimization algorithm in deep learning, and it is used in the DQN algorithm to update the weights of the neural network based on the gradients of the loss function with respect to the parameters, while also incorporating weight decay regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30ade207-f432-443e-9271-74ace1b74ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Get the number of state observation\n",
    "state, info = env.reset()\n",
    "\n",
    "# number of features in the state\n",
    "n_observations = len(state)\n",
    "\n",
    "# target net is initialized with same weight as `policy_net`\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# Optimizer - AdamW used to optimize the weights\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR)\n",
    "\n",
    "# It will store the agent's experiences, which will be used for training.\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "# keep track of number of steps taken by the agent\n",
    "steps_done = 0\n",
    "\n",
    "# input - current state and return an action.\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1 * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return the largest column value of each row.\n",
    "            # second column on max result is index of where max element was found\n",
    "            # so we pick action with the larger expected reward\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample]], device=device, dtype=torch.long)\n",
    "\n",
    "# It is used to keep track of the duration of each episodes\n",
    "episode_durations = []\n",
    "\n",
    "# Plot_durations - visualize the training progress of the DQN \n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    duration_t = torch.tensor(episode_durations, dtype=torch.Float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "\n",
    "    # show the 100 episode moving average of the duration\n",
    "    if(len(durations_t) >= 100):\n",
    "        means = durations_t.unfold(0, 1000, 1).mean(1).view(-1, 1)\n",
    "        means = torch.cat((torch.zeroes(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001) # pause so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "    else:\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1800c256-f1da-4843-9505-3b6205b651f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_mode():\n",
    "    # check if we have enough sample for mini batch\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return \n",
    "\n",
    "    # extract a mini batch of transition(state, action, reward, next_state) from the replay memory\n",
    "    transition = memory.sample(BATCH_SIZE)\n",
    "    # Converts batch-array of Transitions to Transition of batch array\n",
    "    batch = Transition(*zip(*transition))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                           batch.next_state)), device=device, dtype=torch.boolean)\n",
    "\n",
    "    non_final_next_state = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_policy = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeroes(BATCH_SIZE, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_state).max(1)[0]\n",
    "\n",
    "    # Expected Q-values for each transition using the target network\n",
    "    expected_state_action_values = (next_state_values * gamma) + reward_batch\n",
    "\n",
    "    # Compute huber loss\n",
    "    # smooth approximation of the mean square eroor loss less sensitive\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # In-place gradient clipping\n",
    "    # maximum value = 100 to prevent exploding gradient problem.\n",
    "    torch.nn.utils.clip_grad_value_(policy.net.parameters(100))\n",
    "    optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3fab4b74-a70d-4608-83d6-b5a16a28d41c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'method' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(state, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m count():\n\u001b[1;32m---> 12\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     observation, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     14\u001b[0m     reward \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([reward], device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "Cell \u001b[1;32mIn[22], line 38\u001b[0m, in \u001b[0;36mselect_action\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m policy_net(state)\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'method' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 500\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # initialize the environment and get it's state\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # store the transition in memory\n",
    "        memory.push(state, action.next_state, reward)\n",
    "\n",
    "        # move to the next_state\n",
    "        state = next_state\n",
    "\n",
    "        optimize_model()\n",
    "\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = target_net.state_dict()\n",
    "\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key] * TAU + target_net_state_dict * (1 - TAU)\n",
    "\n",
    "        if done:\n",
    "            episodes_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "\n",
    "\n",
    "print('Complete')\n",
    "plot_duration(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46a1291-c121-4351-8ca5-4cb3fcce1dfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41997b9-a98e-4fba-9ff6-f9018e1969c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10020f8a-ccde-407a-a5e4-ac76f436cf5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785f20ae-5230-4aa4-a921-e03632b98a24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RL] *",
   "language": "python",
   "name": "conda-env-RL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
