{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3217716-f8f3-42c9-95e3-358ae9561b02",
   "metadata": {},
   "source": [
    "# 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7caafa-49dc-4f6e-b5fb-588a9f429834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict # Allow access to keys do not access\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch # draw shaped\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gymnasium as gym\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbb820c-7160-4d83-acbd-b310fd398ee4",
   "metadata": {},
   "source": [
    "# Observing the Environment\n",
    "\n",
    "First of all, we call ``env.reset()`` to start an episode. This function\n",
    "resets the environment to a starting position and returns an initial\n",
    "``observation``. We usually also set ``done = False``. This variable\n",
    "will be useful later to check if a game is terminated (i.e., the player wins or loses).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3535b3ab-aaa2-4b20-aebd-3952a572efc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_name = 'Blackjack-v1'\n",
    "env = gym.make(environment_name, sab=True, render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e955b432-0002-446b-86f5-9136be6c536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b72f919-482a-441f-a6a6-c517efac4b7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reset the environment to get the first observation\n",
    "\n",
    "done = False\n",
    "observation, info = env.reset()\n",
    "\n",
    "# observation = (16, 9, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e2bdd0",
   "metadata": {},
   "source": [
    "#### Note that our observation is a tuple consisting of 3 values:\n",
    "1. The player's current sum\n",
    "2. Values of the dealers face-up card\n",
    "3. Boolean whether the player holds a usable ace (it is usale if it counts as 11 without busting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a38930",
   "metadata": {},
   "source": [
    "## Executing an action in the environment\n",
    "\n",
    "After receiving our first observation, we are only going to use the\n",
    "``env.step(action)`` function to interact with the environment. This\n",
    "function takes an action as input and executes it in the environment.\n",
    "Because that action changes the state of the environment, it returns\n",
    "four useful variables to us. These are:\n",
    "\n",
    "-  ``next_state``: This is the observation that the agent will receive\n",
    "   after taking the action.\n",
    "-  ``reward``: This is the reward that the agent will receive after\n",
    "   taking the action.\n",
    "-  ``terminated``: This is a boolean variable that indicates whether or\n",
    "   not the environment has terminated.\n",
    "-  ``truncated``: This is a boolean variable that also indicates whether\n",
    "   the episode ended by early truncation, i.e., a time limit is reached.\n",
    "-  ``info``: This is a dictionary that might contain additional\n",
    "   information about the environment.\n",
    "\n",
    "The ``next_state``, ``reward``,  ``terminated`` and ``truncated`` variables are\n",
    "self-explanatory, but the ``info`` variable requires some additional\n",
    "explanation. This variable contains a dictionary that might have some\n",
    "extra information about the environment, but in the Blackjack-v1\n",
    "environment you can ignore it. For example in Atari environments the\n",
    "info dictionary has a ``ale.lives`` key that tells us how many lives the\n",
    "agent has left. If the agent has 0 lives, then the episode is over.\n",
    "\n",
    "Note that it is not a good idea to call ``env.render()`` in your training\n",
    "loop because rendering slows down training by a lot. Rather try to build\n",
    "an extra loop to evaluate and showcase the agent after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e4214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = env.action_space.sample()\n",
    "# action = 1\n",
    "\n",
    "# execute the action in our environment and receive info after taking the step0\n",
    "observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "#observation = (24, 10, False)\n",
    "#reward = -1.0\n",
    "#terminated = True\n",
    "#truncated = False\n",
    "#info = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a417a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf5aa1c",
   "metadata": {},
   "source": [
    "If terminated=True, we should stop the current episode and begin a new one using env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1bd8b5",
   "metadata": {},
   "source": [
    "### Understand and Implement Epsion-greedy strategy to solve Blackjack\n",
    "\n",
    "1. In this strategy, the agent takes an action that is either the best action based on the current policy (with a probability of 1-epsilon) or a random action (with a probability of epsilon). This approach balances the exploitation of the current best policy with exploration of new policies, which can lead to better rewards in the long run.\n",
    "\n",
    "2. In the context of Blackjack, the epsilon-greedy strategy can be applied to determine whether the player should hit or stand. At each step of the game, the agent(i.e., the player) can choose to take the action that is either recommended by the current policy or a random action. the policy is learned over time by updating the action-value estimates of each state-action pair based on the rewards received during the game.\n",
    "\n",
    "3. As the game is played repeatedly, the agen learns the optimal policy that maximizes the expected reward. Initially, the agent may explore by taking random actions to discover new strategies. However, as the game progresses, the agent will start to exploit the best-known policy, which should maximize the expected reward over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e850a201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent\n",
    "class BlackjackAgent:\n",
    "    def __init__(\n",
    "        self, \n",
    "        learning_rate:float, \n",
    "        initial_epsilon:float,\n",
    "        epsilon_decay:float,\n",
    "        final_epsilon:float,\n",
    "        discount_factor:float=0.95\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize a RL agent with empty dictionary of state-action value (q_value), a learning rate, and an epsilon\n",
    "        \n",
    "        Args:\n",
    "            learning_rate: The learning rate\n",
    "            initial_epsilon: The initial epsilon value\n",
    "            epsilon_decay: The decay for epsilon\n",
    "            final_epsilon: The final epsilon value\n",
    "            discount_factor: The discount factor for computing the Q-value\n",
    "        \"\"\"\n",
    "        self.q_values=defaultdict(lambda:np.zeros(env.action_space.n))\n",
    "        self.lr = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        \n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "        \n",
    "        self.training_error = []\n",
    "        \n",
    "    def get_action(self, obs:tuple[int, int, bool]) -> int:\n",
    "        \"\"\"\n",
    "        Return the best action with probability (1 - epsilon) \n",
    "        otherwise a random action with probability epsilon to ensure exploration.\n",
    "        \"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            return int(np.argmax(self.q_values[obs]))\n",
    "        \n",
    "    def update(self, obs:tuple[int, int, bool], action:int, reward:float, terminated:bool, next_obs:tuple[int, int, bool]):\n",
    "        \"\"\"\n",
    "        Updates the Q-value of an actions\n",
    "        \"\"\"\n",
    "        future_q_value = (not terminated) * np.max(self.q_values[next_obs])\n",
    "        temporal_difference = (reward + self.discount_factor * future_q_value - self.q_values[obs][action])\n",
    "        \n",
    "        self.q_values[obs][action] = self.q_values[obs][action] + self.lr * temporal_difference\n",
    "        \n",
    "        self.training_error.append(temporal_difference)\n",
    "    \n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - epsilon_decay)  \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7a761f",
   "metadata": {},
   "source": [
    "Q-value function is used to estimate the optimal action to take in each state. It is the one that maximized the long-term reward and is given by.\n",
    "\n",
    "To train the agent, we will let the agent play one episode (one complete game is called an episode) at a time and the update it's Q-values after each episode. The agent will have to explerience a lot of episodes to explore the environment sufficiently.\n",
    "\n",
    "Now we should be ready to build the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c66b961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "learning_rate=0.01\n",
    "n_episodes=100_000\n",
    "start_epsilon=1.0\n",
    "epsilon_decay=start_epsilon / (n_episodes / 2) # reduce the exploration over time\n",
    "final_epsilon=0.1\n",
    "\n",
    "agent = BlackjackAgent(\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    final_epsilon=final_epsilon\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531c8a3f",
   "metadata": {},
   "source": [
    "# Training with the Visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7edfbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from IPython.display import clear_output\n",
    "\n",
    "# env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=n_episodes)\n",
    "# for episode in tqdm(range(n_episodes)):\n",
    "#     obs, info = env.reset()\n",
    "#     done = False\n",
    "# #     clear_output()\n",
    "    \n",
    "#     # play one episode:\n",
    "#     while not done:\n",
    "#         action = agent.get_action(obs)\n",
    "#         next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "#         agent.update(obs, action, reward, terminated, next_obs)\n",
    "#         frame = env.render()\n",
    "#         plt.imshow(frame)\n",
    "#         plt.show()\n",
    "        \n",
    "#         done = terminated or truncated\n",
    "#         obs = next_obs\n",
    "        \n",
    "#     agent.decay_epsilon()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59a7401",
   "metadata": {},
   "source": [
    "### Training without visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5464ddd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=n_episodes)\n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "#     clear_output()\n",
    "    \n",
    "    # play one episode:\n",
    "    while not done:\n",
    "        action = agent.get_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        agent.update(obs, action, reward, terminated, next_obs)\n",
    "        \n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "        \n",
    "    agent.decay_epsilon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781a727c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_length = 500\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(12, 5))\n",
    "\n",
    "ax[0].set_title(\"Episode rewards\")\n",
    "reward_moving_average = (\n",
    "    np.convolve(\n",
    "        np.array(env.return_queue).flatten(),\n",
    "        np.ones(rolling_length),\n",
    "        mode='valid'\n",
    "    ) / rolling_length\n",
    ")\n",
    "ax[0].plot(range(len(reward_moving_average)), reward_moving_average)\n",
    "\n",
    "ax[1].set_title('Episode lengths')\n",
    "length_moving_average = (\n",
    "    np.convolve(\n",
    "        np.array(env.length_queue).flatten(),\n",
    "        np.ones(rolling_length),\n",
    "        mode='same'\n",
    "    ) / rolling_length\n",
    ")\n",
    "ax[1].plot(range(len(length_moving_average)), length_moving_average)\n",
    "\n",
    "ax[2].set_title('Training Error')\n",
    "training_error_moving_average = (\n",
    "    np.convolve(\n",
    "        np.array(agent.training_error),\n",
    "        np.ones(rolling_length),\n",
    "        mode='same'\n",
    "    ) / rolling_length\n",
    ")\n",
    "ax[2].plot(range(len(training_error_moving_average)), training_error_moving_average)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b700674",
   "metadata": {},
   "source": [
    "# Visualizing the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea9bd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grids(agent, usable_ace=False):\n",
    "    \"\"\"Create value and policy grid given an agent\"\"\"\n",
    "    # Convert our state-action values to state values\n",
    "    # and build a policy dictionary that maps observations to actions\n",
    "    state_value = defaultdict(float)\n",
    "    policy = defaultdict(int)\n",
    "    \n",
    "    for obs, action_values in agent.q_values.items():\n",
    "        state_value[obs] = float(np.max(action_values))\n",
    "        policy[obs] = int(np.argmax(action_values))\n",
    "        \n",
    "        \n",
    "    player_count, dealer_count = np.meshgrid(\n",
    "        # players count, dealers face-up card\n",
    "        np.arange(12, 22),\n",
    "        np.arange(1, 11)\n",
    "    )\n",
    "    \n",
    "    # Create the value grid for plotting\n",
    "    value = np.apply_along_axis(\n",
    "        lambda obs: state_value[(obs[0], obs[1], usable_ace)], \n",
    "        axis=2,\n",
    "        arr = np.dstack([player_count, dealer_count])\n",
    "    )\n",
    "    value_grid = player_count, dealer_count, value\n",
    "    \n",
    "    # Create the policy grid for plotting\n",
    "    policy_grid = np.apply_along_axis(\n",
    "        lambda obs: policy[(obs[0], obs[1], usable_ace)],\n",
    "        axis = 2,\n",
    "        arr = np.dstack([player_count, dealer_count])\n",
    "    )\n",
    "    \n",
    "    return value_grid, policy_grid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff864b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_plots(value_grid, policy_grid, title: str):\n",
    "    \"\"\"Create a plot using a value and policy grid\"\"\"\n",
    "    # Create a new figure with 2 subplots (left: state_values, right: policy)\n",
    "    player_count, dealer_count, value = value_grid\n",
    "    fig = plt.figure(figsize=plt.figaspect(0.4))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    \n",
    "    # Plot the state values\n",
    "    ax1 = fig.add_subplot(1, 2, 1, projection=\"3d\")\n",
    "    ax1.plot_surface(\n",
    "        player_count,\n",
    "        dealer_count,\n",
    "        value,\n",
    "        rstride=1,\n",
    "        cstride=1,\n",
    "        cmap='viridis',\n",
    "        edgecolor='none'\n",
    "    )\n",
    "    \n",
    "    plt.xticks(range(12, 22), range(12, 22))\n",
    "    plt.yticks(range(1, 11), ['A'] + list(range(2, 11)))\n",
    "    ax1.set_title(f'State values: {title}')\n",
    "    ax1.set_xlabel('Player sum')\n",
    "    ax1.set_ylabel('Dealer showing')\n",
    "    ax1.zaxis.set_rotate_label(False)\n",
    "    ax1.set_zlabel('value', fontsize=14, rotation=90)\n",
    "    ax1.view_init(20, 220)\n",
    "    \n",
    "    \n",
    "    # Plot the policy\n",
    "    fig.add_subplot(1, 2, 2)\n",
    "    ax2 = sns.heatmap(policy_grid, linewidth=0, annot=True, cmap='Accent_r', cbar=False)\n",
    "    ax2.set_title(f'Policy: {title}')\n",
    "    ax2.set_xlabel('Player sum')\n",
    "    ax2.set_ylabel('Dealer showing')\n",
    "    ax2.set_xticklabels(range(12, 22))\n",
    "    ax2.set_yticklabels(['A'] + list(range(2, 11)), fontsize=12)\n",
    "    \n",
    "    \n",
    "    # Add a legent\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='lightgreen', edgecolor='black', label='Hit'),\n",
    "        Patch(facecolor='grey', edgecolor='black', label='Stick')\n",
    "    ]\n",
    "    ax2.legend(handles=legend_elements, bbox_to_anchor=(1.3, 1))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6363dc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State values & policy with usabel ace (ace counts as 11)\n",
    "value_grid, policy_grid = create_grids(agent, usable_ace=True)\n",
    "fig1 = create_plots(value_grid, policy_grid, title='With usable ace')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03de79ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State values & policy without usable ace (ace count as 1)\n",
    "value_grid, policy_grid = create_grids(agent, usable_ace=False)\n",
    "fig2 = create_plots(value_grid, policy_grid, title='Without usable ace')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fce58f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df200885",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fb9721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc81b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc5dec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0635e104",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
